{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Data wrangling for files describing the Pacific and Atlantic hurricane tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the data files presented by the Department of the Interior of US Geological Survey, that describe the tracks of historic hurricanes originated in Pacific and Atlantic oceans. The datasets were downloaded from the webpages:\n",
    "\n",
    "https://catalog.data.gov/dataset/historical-north-atlantic-tropical-cyclone-tracks-1851-2004-direct-download\n",
    "\n",
    "https://catalog.data.gov/dataset/historical-eastern-north-pacific-tropical-cyclone-tracks-1949-2004-direct-download\n",
    "\n",
    "We first import all necessary modules including the .dbf file reader DBF5 to obtain the Pacific and Atlantic hurricane tracks dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the .dbf file reader\n",
    "from simpledbf import Dbf5\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gpxpy.geo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read the tracks files into the dataframes df_Atl and df_Pac. Running .info() indicates that these files contain no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19381 entries, 0 to 19380\n",
      "Data columns (total 17 columns):\n",
      "FNODE_        19381 non-null int64\n",
      "TNODE_        19381 non-null int64\n",
      "LPOLY_        19381 non-null int64\n",
      "RPOLY_        19381 non-null int64\n",
      "LENGTH        19381 non-null float64\n",
      "PHRALLL020    19381 non-null int64\n",
      "YEAR          19381 non-null int64\n",
      "MONTH         19381 non-null int64\n",
      "DAY           19381 non-null int64\n",
      "BTID          19381 non-null int64\n",
      "NAME          19381 non-null object\n",
      "LONG          19381 non-null float64\n",
      "LAT           19381 non-null float64\n",
      "WIND_KTS      19381 non-null float64\n",
      "PRESSURE      19381 non-null int64\n",
      "WIND_MPH      19381 non-null float64\n",
      "CATEGORY      19381 non-null object\n",
      "dtypes: float64(5), int64(10), object(2)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Pacific hurricanes\n",
    "\n",
    "dbf1 = Dbf5('phralll020.dbf')\n",
    "df_Pac = dbf1.to_dataframe()\n",
    "\n",
    "#df_Pac.head(30)\n",
    "#df_Pac.tail(20)\n",
    "df_Pac.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37706 entries, 0 to 37705\n",
      "Data columns (total 17 columns):\n",
      "FNODE_        37706 non-null int64\n",
      "TNODE_        37706 non-null int64\n",
      "LPOLY_        37706 non-null int64\n",
      "RPOLY_        37706 non-null int64\n",
      "LENGTH        37706 non-null float64\n",
      "HURALLL020    37706 non-null int64\n",
      "YEAR          37706 non-null int64\n",
      "MONTH         37706 non-null int64\n",
      "DAY           37706 non-null int64\n",
      "BTID          37706 non-null int64\n",
      "NAME          37706 non-null object\n",
      "LONG          37706 non-null float64\n",
      "LAT           37706 non-null float64\n",
      "WIND_KTS      37706 non-null float64\n",
      "PRESSURE      37706 non-null int64\n",
      "WIND_MPH      37706 non-null float64\n",
      "CATEGORY      37706 non-null object\n",
      "dtypes: float64(5), int64(10), object(2)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Atlantic hurricanes\n",
    "\n",
    "dbf2 = Dbf5('huralll020.dbf')\n",
    "df_Atl = dbf2.to_dataframe()\n",
    "\n",
    "#df_Atl.head(20)\n",
    "#df_Atl.tail(20)\n",
    "df_Atl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the hurricanes started receiving names only after 1949, and only after that year the data are more or less accurate, we will limit ourselves to hurricanes that occurred in 1950 or later. If a hurricane after 1949 is still not named, we will remove it from the database as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_Atl = df_Atl[df_Atl['YEAR'] >= 1950]\n",
    "df_Pac = df_Pac[df_Pac['YEAR'] >= 1950]\n",
    "\n",
    "df_Atl = df_Atl[df_Atl['NAME'] != 'NOT NAMED']\n",
    "df_Pac = df_Pac[df_Pac['NAME'] != 'NOT NAMED']\n",
    "\n",
    "\n",
    "#df_Atl.head(20)\n",
    "#df_Pac.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will limit ourselves only with the storms that can be considered dangerous. We remove from the database the low-pressure systems, named 'SUBTROP', 'SUBTROP 2', 'SUBTROP 3', 'SUBTROP 4', that could not reach the dangerous limit, and consider only the depressions, storms (both subtropical and tropical) and hurricanes of all five categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TS', 'H1', 'H2', 'H3', 'H4', 'TD', 'H5', 'SD', 'SS'], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Atl = df_Atl[~df_Atl['NAME'].isin(['SUBTROP', 'SUBTROP 2', 'SUBTROP 3', 'SUBTROP 4'])]\n",
    "\n",
    "df_Atl = df_Atl[df_Atl['CATEGORY'].isin(['TD', 'SD', 'SS', 'TS', 'H1', 'H2', 'H3', 'H4', 'H5'])]\n",
    "df_Pac = df_Pac[df_Pac['CATEGORY'].isin(['TD', 'SD', 'SS', 'TS', 'H1', 'H2', 'H3', 'H4', 'H5'])]\n",
    "\n",
    "\n",
    "df_Pac['CATEGORY'].unique()\n",
    "df_Atl['CATEGORY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframes df_Atl and df_Pac contain many columns, but we will keep only the columns describing year, month, day, name, longitude of the center, latitude of the center and category of each hurricane. We will omit the columns describing internal pressure and sustained wind since they themselves determine the category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15039 entries, 19969 to 37694\n",
      "Data columns (total 7 columns):\n",
      "YEAR        15039 non-null int64\n",
      "MONTH       15039 non-null int64\n",
      "DAY         15039 non-null int64\n",
      "NAME        15039 non-null object\n",
      "LONG        15039 non-null float64\n",
      "LAT         15039 non-null float64\n",
      "CATEGORY    15039 non-null object\n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 939.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17656 entries, 129 to 19380\n",
      "Data columns (total 7 columns):\n",
      "YEAR        17656 non-null int64\n",
      "MONTH       17656 non-null int64\n",
      "DAY         17656 non-null int64\n",
      "NAME        17656 non-null object\n",
      "LONG        17656 non-null float64\n",
      "LAT         17656 non-null float64\n",
      "CATEGORY    17656 non-null object\n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_Atl = df_Atl[['YEAR', 'MONTH', 'DAY', 'NAME', 'LONG', 'LAT', 'CATEGORY']]\n",
    "df_Pac = df_Pac[['YEAR', 'MONTH', 'DAY', 'NAME', 'LONG', 'LAT', 'CATEGORY']]\n",
    "\n",
    "df_Atl.info()\n",
    "df_Pac.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to assign an integer number ranging from 0 to 6 to each category of a hurricane based on its strength. \n",
    "The weakest low pressure system, depression, is given 0, while the strongest one, category five hurricane, is assigned 6. \n",
    "The new column is called 'EFFECTIVE STRENGTH'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LONG</th>\n",
       "      <th>LAT</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>EFFECTIVE STRENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19969</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-54.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>TS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-55.5</td>\n",
       "      <td>17.1</td>\n",
       "      <td>TS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>TS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-58.3</td>\n",
       "      <td>18.4</td>\n",
       "      <td>TS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-59.5</td>\n",
       "      <td>19.1</td>\n",
       "      <td>TS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-61.1</td>\n",
       "      <td>20.1</td>\n",
       "      <td>TS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-62.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>H1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-63.2</td>\n",
       "      <td>21.6</td>\n",
       "      <td>H1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-63.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>H1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>1950</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-64.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>H1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       YEAR  MONTH  DAY  NAME  LONG   LAT CATEGORY  EFFECTIVE STRENGTH\n",
       "19969  1950      8   12  ABLE -54.5  16.5       TS                   1\n",
       "19970  1950      8   12  ABLE -55.5  17.1       TS                   1\n",
       "19971  1950      8   12  ABLE -57.0  17.7       TS                   1\n",
       "19972  1950      8   12  ABLE -58.3  18.4       TS                   1\n",
       "19973  1950      8   13  ABLE -59.5  19.1       TS                   1\n",
       "19974  1950      8   13  ABLE -61.1  20.1       TS                   1\n",
       "19975  1950      8   13  ABLE -62.5  21.0       H1                   2\n",
       "19976  1950      8   13  ABLE -63.2  21.6       H1                   2\n",
       "19977  1950      8   14  ABLE -63.7  22.2       H1                   2\n",
       "19978  1950      8   14  ABLE -64.6  23.0       H1                   2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary assigning a number from 0 to 6 to each category based on the strength of a hurricane\n",
    "\n",
    "categ_dict = {'SD':0, 'TD':0, 'SS':1, 'TS':1, 'H1':2, 'H2':3, 'H3':4, 'H4':5, 'H5':6}\n",
    "\n",
    "\n",
    "# Create a new column 'EFFECTIVE STRENGTH' that contains the number assigned to each category\n",
    "\n",
    "df_Atl['EFFECTIVE STRENGTH'] = df_Atl['CATEGORY'].map(categ_dict)\n",
    "df_Pac['EFFECTIVE STRENGTH'] = df_Pac['CATEGORY'].map(categ_dict)\n",
    "\n",
    "df_Pac.tail(10)\n",
    "df_Atl.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the future analysis, we will create a table that contains: 1) the first longitude and latitude when hurricane is dangerous, 2) the last longitude or latitude when hurricane is still dangerous (or before making a landfall), 3) the total time travelled in hours, 4) the total distance travelled (in km), and 5) the maximum effective strength. We first define the auxiliary functions that help to compute us 1) through 4). The total distance is calculated using the Haversine formula \n",
    "from the imported gpxy module, while the total number of hours is computed by taking the number of entries for a given hurricane and multiplying by 6 hrs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# returns the first longitude or latitude when hurricane is dangerous \n",
    "def first(series):\n",
    "    return series.iloc[0]\n",
    "\n",
    "# returns the last longitude or latitude when hurricane is still dangerous \n",
    "def last(series):\n",
    "    return series.iloc[-1]\n",
    "\n",
    "\n",
    "# returns the approximate total time travelled by a hurricane (in hours) when it is still dangerous\n",
    "def time_travelled(series):\n",
    "    return 6.0*len(series)\n",
    "\n",
    "# returns the approximate distance travelled (in km) using Haversine formula\n",
    "def dist_travelled(gr):\n",
    "    dist = 0\n",
    "    for i in range(len(gr['LAT'])-1):\n",
    "        dist += gpxpy.geo.haversine_distance(gr['LAT'].iloc[i], gr['LONG'].iloc[i], gr['LAT'].iloc[i+1], \n",
    "                                             gr['LONG'].iloc[i+1])\n",
    "    return dist/1000   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we use the combination of groupby and merging technique to create the new table containing the aforementioned information for both Pacific and Atlantic hurricanes. The new data frame for Atlantic hurricanes contains 527 rows, while\n",
    "the data frame for Pacific hurricanes 679 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>NAME</th>\n",
       "      <th>FIRST LONG</th>\n",
       "      <th>FIRST LAT</th>\n",
       "      <th>LAST LONG</th>\n",
       "      <th>LAST LAT</th>\n",
       "      <th>DISTANCE TRAVELLED(KM)</th>\n",
       "      <th>TIME TRAVELLED(HRS)</th>\n",
       "      <th>MAXIMUM EFFECTIVE STRENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950</td>\n",
       "      <td>ABLE</td>\n",
       "      <td>-54.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>-50.7</td>\n",
       "      <td>53.6</td>\n",
       "      <td>6235.232439</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1950</td>\n",
       "      <td>BAKER</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>-89.9</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5214.709501</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1950</td>\n",
       "      <td>CHARLIE</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-58.1</td>\n",
       "      <td>38.4</td>\n",
       "      <td>6707.753820</td>\n",
       "      <td>348</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950</td>\n",
       "      <td>DOG</td>\n",
       "      <td>-55.3</td>\n",
       "      <td>15.2</td>\n",
       "      <td>-70.6</td>\n",
       "      <td>39.3</td>\n",
       "      <td>3900.519360</td>\n",
       "      <td>300</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1950</td>\n",
       "      <td>EASY</td>\n",
       "      <td>-84.1</td>\n",
       "      <td>19.1</td>\n",
       "      <td>-90.2</td>\n",
       "      <td>35.9</td>\n",
       "      <td>2735.355281</td>\n",
       "      <td>210</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR     NAME  FIRST LONG  FIRST LAT  LAST LONG  LAST LAT  \\\n",
       "0  1950     ABLE       -54.5       16.5      -50.7      53.6   \n",
       "1  1950    BAKER       -55.0       16.3      -89.9      37.0   \n",
       "2  1950  CHARLIE       -24.0       13.1      -58.1      38.4   \n",
       "3  1950      DOG       -55.3       15.2      -70.6      39.3   \n",
       "4  1950     EASY       -84.1       19.1      -90.2      35.9   \n",
       "\n",
       "   DISTANCE TRAVELLED(KM)  TIME TRAVELLED(HRS)  MAXIMUM EFFECTIVE STRENGTH  \n",
       "0             6235.232439                  264                           5  \n",
       "1             5214.709501                  306                           4  \n",
       "2             6707.753820                  348                           4  \n",
       "3             3900.519360                  300                           6  \n",
       "4             2735.355281                  210                           4  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Atl_aux1 = pd.DataFrame(df_Atl.groupby(['YEAR', 'NAME'])['LONG', 'LAT'].agg({'LONG':first, \n",
    "         'LAT':first})).rename( columns={'LONG': 'FIRST LONG', 'LAT': 'FIRST LAT'}).reset_index()\n",
    "\n",
    "df_Atl_aux2 = pd.DataFrame(df_Atl.groupby(['YEAR', 'NAME'])['LONG', 'LAT'].agg({'LONG':last, \n",
    "         'LAT':last})).rename( columns={'LONG': 'LAST LONG', 'LAT': 'LAST LAT'}).reset_index()\n",
    "\n",
    "df_Atl_aux3 = pd.DataFrame(df_Atl.groupby(['YEAR', 'NAME'])['DAY', 'EFFECTIVE STRENGTH'].\\\n",
    "         agg({'DAY':time_travelled, 'EFFECTIVE STRENGTH': 'max'}))  \\\n",
    "        .rename( columns={'DAY': 'TIME TRAVELLED(HRS)', 'EFFECTIVE STRENGTH': 'MAXIMUM EFFECTIVE STRENGTH'}).reset_index()\n",
    "\n",
    "df_Atl_aux4 = pd.DataFrame({'DISTANCE TRAVELLED(KM)': df_Atl.groupby(['YEAR', 'NAME']).apply(dist_travelled)}).reset_index()\n",
    "\n",
    "df_Atl_aux5 = pd.merge(df_Atl_aux1, df_Atl_aux2, on = ['YEAR', 'NAME'])\n",
    "\n",
    "df_Atl_aux6 = pd.merge(df_Atl_aux4, df_Atl_aux3, on = ['YEAR', 'NAME'])\n",
    "\n",
    "df_Atl_new = pd.merge(df_Atl_aux5, df_Atl_aux6, on = ['YEAR', 'NAME'])\n",
    "\n",
    "df_Atl_new.head()\n",
    "#df_Pac_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>NAME</th>\n",
       "      <th>FIRST LONG</th>\n",
       "      <th>FIRST LAT</th>\n",
       "      <th>LAST LONG</th>\n",
       "      <th>LAST LAT</th>\n",
       "      <th>DISTANCE TRAVELLED(KM)</th>\n",
       "      <th>TIME TRAVELLED(HRS)</th>\n",
       "      <th>MAXIMUM EFFECTIVE STRENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950</td>\n",
       "      <td>HIKI</td>\n",
       "      <td>-144.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>-178.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>4042.790225</td>\n",
       "      <td>234</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1957</td>\n",
       "      <td>DELLA</td>\n",
       "      <td>-149.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>163.5</td>\n",
       "      <td>24.8</td>\n",
       "      <td>5609.392990</td>\n",
       "      <td>294</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1957</td>\n",
       "      <td>KANOA</td>\n",
       "      <td>-108.2</td>\n",
       "      <td>11.9</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>5041.538137</td>\n",
       "      <td>270</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1957</td>\n",
       "      <td>NINA</td>\n",
       "      <td>-162.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>-173.3</td>\n",
       "      <td>17.7</td>\n",
       "      <td>2637.732151</td>\n",
       "      <td>186</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1959</td>\n",
       "      <td>DOT</td>\n",
       "      <td>-141.2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>-163.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>2712.404781</td>\n",
       "      <td>162</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR   NAME  FIRST LONG  FIRST LAT  LAST LONG  LAST LAT  \\\n",
       "0  1950   HIKI      -144.5       14.5     -178.0      27.4   \n",
       "1  1957  DELLA      -149.6       15.0      163.5      24.8   \n",
       "2  1957  KANOA      -108.2       11.9     -154.0      19.7   \n",
       "3  1957   NINA      -162.5       11.7     -173.3      17.7   \n",
       "4  1959    DOT      -141.2       15.7     -163.0      23.2   \n",
       "\n",
       "   DISTANCE TRAVELLED(KM)  TIME TRAVELLED(HRS)  MAXIMUM EFFECTIVE STRENGTH  \n",
       "0             4042.790225                  234                           2  \n",
       "1             5609.392990                  294                           4  \n",
       "2             5041.538137                  270                           2  \n",
       "3             2637.732151                  186                           2  \n",
       "4             2712.404781                  162                           5  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Pac_aux1 = pd.DataFrame(df_Pac.groupby(['YEAR', 'NAME'])['LONG', 'LAT'].agg({'LONG':first, \n",
    "         'LAT':first})).rename( columns={'LONG': 'FIRST LONG', 'LAT': 'FIRST LAT'}).reset_index()\n",
    "\n",
    "df_Pac_aux2 = pd.DataFrame(df_Pac.groupby(['YEAR', 'NAME'])['LONG', 'LAT'].agg({'LONG':last, \n",
    "         'LAT':last})).rename( columns={'LONG': 'LAST LONG', 'LAT': 'LAST LAT'}).reset_index()\n",
    "\n",
    "df_Pac_aux3 = pd.DataFrame(df_Pac.groupby(['YEAR', 'NAME'])['DAY', 'EFFECTIVE STRENGTH'].\\\n",
    "         agg({'DAY':time_travelled, 'EFFECTIVE STRENGTH': 'max'}))  \\\n",
    "        .rename( columns={'DAY': 'TIME TRAVELLED(HRS)', 'EFFECTIVE STRENGTH': 'MAXIMUM EFFECTIVE STRENGTH'}).reset_index()\n",
    "\n",
    "df_Pac_aux4 = pd.DataFrame({'DISTANCE TRAVELLED(KM)': df_Pac.groupby(['YEAR', 'NAME']).apply(dist_travelled)}).reset_index()\n",
    "\n",
    "df_Pac_aux5 = pd.merge(df_Pac_aux1, df_Pac_aux2, on = ['YEAR', 'NAME'])\n",
    "\n",
    "df_Pac_aux6 = pd.merge(df_Pac_aux4, df_Pac_aux3, on = ['YEAR', 'NAME'])\n",
    "\n",
    "df_Pac_new = pd.merge(df_Pac_aux5, df_Pac_aux6, on = ['YEAR', 'NAME'])\n",
    "\n",
    "df_Pac_new.head()\n",
    "#df_Pac_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the combined list of the names of the hurricanes (Atlantic and Pacific) to be used in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_Atl = list(df_Atl['NAME'].unique())\n",
    "names_Pac = list(df_Pac['NAME'].unique())\n",
    "\n",
    "merged_names_list = list(set(names_Atl + names_Pac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data wrangling for 'Emergencies_database.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will do the data preparation for the csv-file 'Emergencies_database.csv' taken from the FEMA open source database.\n",
    "Contrary to another interesting data file 'DisasterDeclarationsSummaries.csv', this file contains information about all counties in all states where the state of emergency because of a hurricane was declared. Counting the counties affected by disastrous hurricanes is much more informative than counting just the states. The file can be downloaded from\n",
    "\n",
    "https://www.fema.gov/openfema-dataset-disaster-declarations-summaries-v1\n",
    "\n",
    "We first select the rows with the column 'Disaster Type' equal to 'Hurricane' and 'Typhoon' from 'Emergencies_database.csv' that initially had 46185 rows. Now the dataframe has 8883 rows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8883 entries, 20 to 46059\n",
      "Data columns (total 14 columns):\n",
      "Declaration Number                  8883 non-null object\n",
      "Declaration Type                    8883 non-null object\n",
      "Declaration Date                    8883 non-null object\n",
      "State                               8883 non-null object\n",
      "County                              8847 non-null object\n",
      "Disaster Type                       8883 non-null object\n",
      "Disaster Title                      8883 non-null object\n",
      "Start Date                          8883 non-null object\n",
      "End Date                            8883 non-null object\n",
      "Close Date                          6189 non-null object\n",
      "Individual Assistance Program       8883 non-null object\n",
      "Individuals & Households Program    8883 non-null object\n",
      "Public Assistance Program           8883 non-null object\n",
      "Hazard Mitigation Program           8883 non-null object\n",
      "dtypes: object(14)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_emerg = pd.read_csv('Emergencies_database.csv', parse_dates = True)\n",
    "#df_emerg.info()\n",
    "#df_emerg['Disaster Type'].unique()\n",
    "\n",
    "df_emerg = df_emerg[df_emerg['Disaster Type'].isin(['Hurricane', 'Typhoon'])]\n",
    "df_emerg.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose the columns 'Declaration Date', 'State', 'County', 'Disaster Title' that are only interesting to us, and look at the missing values. Some values are missing from the 'County' column, and we fill them with the value 'Some name' regarding each such entry as a distinct county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8883 entries, 20 to 46059\n",
      "Data columns (total 4 columns):\n",
      "Declaration Date    8883 non-null object\n",
      "State               8883 non-null object\n",
      "County              8883 non-null object\n",
      "Disaster Title      8883 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 347.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_emerg = df_emerg[['Declaration Date', 'State', 'County', 'Disaster Title']]\n",
    "df_emerg.fillna('Some name',inplace = True)\n",
    "\n",
    "df_emerg.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then recast the 'Declaration Date' column in the standard datetime format and create three separate columns,\n",
    "corresponding to year, month and day with the names to match the hurricane tracks dataframes. There are no missing or incorrectly entered data in these columns. The earliest year is 1954 and the latest is 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964,\n",
       "       1965, 1966, 1967, 1968, 1969, 1970, 1971, 1974, 1976, 1979, 1980,\n",
       "       1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991,\n",
       "       1992, 1993, 1995, 1996, 1997, 1998, 1999, 2002, 2003, 2004, 2005,\n",
       "       2007, 2008, 2009, 2010, 2011, 2012, 2013, 2015, 2016], dtype=int64)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emerg['Declaration Date'] = pd.to_datetime(df_emerg['Declaration Date'])\n",
    "\n",
    "df_emerg['YEAR'] = df_emerg['Declaration Date'].dt.year\n",
    "df_emerg['MONTH'] = df_emerg['Declaration Date'].dt.month\n",
    "df_emerg['DAY'] = df_emerg['Declaration Date'].dt.day\n",
    "\n",
    "df_emerg.MONTH.unique()\n",
    "df_emerg.DAY.unique()\n",
    "df_emerg.YEAR.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of values in the column 'Disaster Title' contains the name of a hurricane. Thus, our next task will be to single out the name of a hurricane in each entry of the column. To do this, we capitalize the entry and remove everything that does not contain a name from the combined list of names for Atlantic and Pacific hurricanes created from the hurricane tracks dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8883 entries, 20 to 46059\n",
      "Data columns (total 7 columns):\n",
      "Declaration Date    8883 non-null datetime64[ns]\n",
      "State               8883 non-null object\n",
      "County              8883 non-null object\n",
      "Disaster Title      8883 non-null object\n",
      "YEAR                8883 non-null int64\n",
      "MONTH               8883 non-null int64\n",
      "DAY                 8883 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(3), object(3)\n",
      "memory usage: 555.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Capitalize all words in the column 'Disaster Title'\n",
    "df_aux = df_emerg['Disaster Title'].str.upper().str.split()\n",
    "\n",
    "#remove all words that do not belong to names \n",
    "for ind in df_aux.index:\n",
    "    df_aux[ind] = ''.join([word for word in df_aux[ind] if word in merged_names_list])\n",
    "\n",
    "df_emerg['Disaster Title'] = pd.DataFrame(df_aux)\n",
    "\n",
    "df_emerg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataframe contains 8883 rows but 1664 rows are empty and do not contain the name of a hurricane. We will drop those\n",
    "rows and rename the name of the column 'Disaster Title' to name. The new dataframe contains 7219 rows, but examining it further we see that the sensible data about the number of counties affected is for hurricanes dated by 1965 and later. Thus we slightly trim our dataframe to arrive at the new dataframe containing 7207 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7207 entries, 665 to 46059\n",
      "Data columns (total 7 columns):\n",
      "Declaration Date    7207 non-null datetime64[ns]\n",
      "State               7207 non-null object\n",
      "County              7207 non-null object\n",
      "NAME                7207 non-null object\n",
      "YEAR                7207 non-null int64\n",
      "MONTH               7207 non-null int64\n",
      "DAY                 7207 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(3), object(3)\n",
      "memory usage: 450.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#df_emerg[df_emerg['Disaster Title'] == '']['Disaster Title'].value_counts()\n",
    "\n",
    "df_emerg = df_emerg.replace('', np.nan).dropna()\n",
    "df_emerg = df_emerg.rename(columns = {'Disaster Title': 'NAME'})\n",
    "\n",
    "#df_emerg.head(100)\n",
    "#df_emerg.info()\n",
    "\n",
    "df_emerg = df_emerg[df_emerg['YEAR'] >= 1965]\n",
    "\n",
    "df_emerg.head(100)\n",
    "df_emerg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will count the number of affected counties grouping them by year and name. The new dataframe is called\n",
    "df_emerg_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68 entries, 0 to 67\n",
      "Data columns (total 3 columns):\n",
      "YEAR                  68 non-null int64\n",
      "NAME                  68 non-null object\n",
      "NUMBER OF COUNTIES    68 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_emerg_new = df_emerg.groupby(['YEAR', 'NAME'])['County'].count()\n",
    "\n",
    "df_emerg_new = pd.DataFrame(df_emerg_new.reset_index())\n",
    "df_emerg_new = df_emerg_new.rename(columns = {'County':'NUMBER OF COUNTIES'})\n",
    "\n",
    "# df_emerg_new\n",
    "df_emerg_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
